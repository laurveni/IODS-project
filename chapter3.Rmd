#Logistic Regression

The joined data set that we will use combines two data sets (student-mat and student-por) that approach student achievement in secondary education of two Portuguese schools. [Here](https://archive.ics.uci.edu/ml/datasets/Student+Performance) you can find more information about the data.
We made the following adjustments:

* the variables not used for joining the two data have been combined by averaging (including the grade variables);
* 'alc_use' is the average of 'Dalc' and 'Walc';
* 'high_use' is TRUE if 'alc_use' is higher than 2 and FALSE otherwise.

This is the dimension of the data set.
```{r dimensiona}
alc <- read.csv(file="data/student-alc.csv")
dim(alc)
```

This is the structure.
```{r structure1}
str(alc)
```

Let us look at the distributions of the variables.
```{r graph1}
library(tidyr); library(dplyr); library(ggplot2)
gather(alc) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free")+geom_bar()
```

We make the hypothesis that there is a relationship between alcohol consumption (high_use) and the final grade (G3): high grades correspond to low alcohol consumption and low grades to high alcohol consumption. Similarly, we make the hypthesis of a correlation between a high number of classes failed (failures) and high alcohol consumption. Also a high number of absences from school could correspond to a high alcohol consumption. Moreover, it seems plausible that male students consume more alcohol than females. 

Let us look at a bar plot of high_use by sex.
```{r plota,}
g2 <- ggplot(data = alc, aes(x=high_use))
g2+ geom_bar()+ facet_wrap("sex")
```

We can see that less than one third of females students has high alcohol consumption, whereas for male students it is more than half. Thus our hypthesis about a correlation between sex and alcohol consumption seems reasonable.


Looking at this table
```{r crosstables}
alc %>% group_by(sex,high_use) %>% summarise(count = n(),mean_grade=mean(G3))
```

and this box-plot,

```{r boxplot1}
g1 <- ggplot(alc, aes(x = high_use, y = G3,col=sex))
g1 + geom_boxplot() + ylab("grade")
```

it appears to be reasonable that for male students low alcohol consumption corresponds to higher grades, whereas for female students it is not so clear.

Let us take a look at the relationship between failures and high_use. The variable failures takes value n if n is the number of failed classes and n is at most 3, whereas it takes the value 4 if the number of failed classes is at least 4.

```{r boxplot3}
g2 <- ggplot(data = alc, aes(x=high_use))
g2+ geom_bar()+ facet_wrap("failures")

```

More than half of the students who have not failed any class have low alcohol consumption so our hypothesis seems reasonable.

The following box-plot suggests that a higher number of school absences corresponds to a higher alcohol consumption.

```{r boxplot2}
g2 <- ggplot(alc,aes(x=high_use,y=absences,col=sex))
g2+geom_boxplot()

```



We now fit a logistic regression model with high_use as the target variable and the final grade (G3), failures, absences and sex as explanatory variables. 

```{r modela}
m <- glm(high_use ~ G3 + absences+sex+failures, data = alc, family = "binomial")
summary(m)
```

The p-values of absences, sex and failures are very low so these variables have a statistically signifant relationship with the variable high_use. The p-value of G3, instead, is close to 0.2 so we exclude it and fit a new model with only the other three variables as explanatory ones. Here is a summary of the model.



```{r modela2}
m2 <- glm(high_use ~ failures + absences + sex, data = alc, family = "binomial")
summary(m2)
```

Let us take a look at how well the second model can make predictions. Here is a table of predictions versus the actual values of the variable high_use.
```{r prob}
probabilities <- predict(m2, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)

# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = probability>0.5)

# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)
```

And here is the avarage number of wrong predictions in the training data.

```{r error}
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = alc$probability)
```

If we assume that the probability of high_use is zero for each individual (so we guess that high_use is always FALSE) then the avarage number of wrong predictions is 

```{r error2}
loss_func(class = alc$high_use, prob = 0)
```

which is a bit higher than that of our model. Thus our model can predict a bit better than a simple guessing strategy (even if not much better).

Let us now perform a 10-fold cross-validation on the model. The avarage number of wrong predictions in the cross validation is

```{r validation}
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```