#Clustering and classification

We will work with the Boston data set, which contains data related to housing characteristics of the suburbs of Boston, such as the per capita crime rate by town (column "crim"). [Here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) you can find more information about the data set.

Let us now look at the dimensions and structure of the data set.

```{r}
library(MASS)
data("Boston")
dim(Boston)
str(Boston)
```

In order to investigate how the variables are related to each other, we visualize the correlation matrix. Positive correlations are displayed in blue and negative correlations in red color. Color intensity and the size of the circle are proportional to the correlation coefficients.

```{r}
library(dplyr)
library(corrplot)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)
```

We can see, for example, that the crime rate has quite strong positive correlation with the index of accessibility to radial highways (rad) and the full-value property-tax rate per $10,000 (tax). 

Here is a summary of the variables.

```{r}
summary(Boston)
```

**Linear Discriminant Analysis**

Since we will perform Linear Discriminant Analysis (LDA) on the data frame, we scale it so that the mean of each variable will be zero and the standard deviation one. Here is a summary of the scaled variables.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
```

Moreover, since the target variable in LDA needs to be categorical, we create a categorical variable of the crime rate (from the scaled crime rate), using the quantiles as the break points in the categorical variable. We drop the old crime rate variable from the dataset and add the new categorical value to the scaled data set. Here you can see the table of the new variable crime.

```{r}
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# create a quantile vector of crim 
bins <- quantile(boston_scaled$crim)
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low","med_low", "med_high", "high"))
# look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Next, we divide randomly the dataset into train and test sets, so that 80% of the data belongs to the train set. 

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

We now fit the Linear Discriminant Analysis on the train set, using the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. You can see here the LDA model and the LDA (bi)plot.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ . , data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 2)

```

The most influential variables are those with longer arrows in the plot, so rad, nox (nitrogen oxides concentration) and zn (proportion of residential land zoned for lots over 25,000 sq.ft).
We now predict the classes with the LDA model on the test data and cross tabulate the results with the crime categories from the test set. 

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

We can see that the predictions are quite accurate, even if not completely. Roughly half of the data that is supposed to be in the category "low" is in it, and the remaining is mostly in "medium low" so not too far. The category with high crime rate is the one that is predicted best.

**K-means**

We now reload the Boston dataset and standardize the dataset, scaling the variables to get comparable distances. We display a summary of the Euclidean distances between the observations. 

```{r}
library(MASS)
data('Boston')
boston_s <- scale(Boston)
boston_s <- as.data.frame(boston_s)
# euclidean distance matrix
dist_eu <- dist(boston_s)
# look at the summary of the distances
summary(dist_eu)
```

We run the k-means algorithm, which is a clustering method, on the dataset, choosing 4 as the number of clusters. We visualize the clusters with different colors, looking at the variables rm (average number of rooms per dwelling), age (proportion of owner-occupied units built prior to 1940), dis (weighted mean of distances to five Boston employment centres.), rad and tax. 

```{r}
# k-means clustering with 4 clusters
km <-kmeans(boston_s, centers = 4)
# plot the boston_s dataset with clusters
pairs(boston_s[6:10], col = km$cluster)

```

To investigate what is the optimal number of clusters, we look at how the total of within cluster sum of squares (WCSS) behaves when the number of clusters changes. We show the plot of the number of clusters and the total WCSS: the optimal number of clusters is that corresponding to a quick drop in the total WCSS. In this case two clusters seem to be optimal.


```{r}
library(ggplot2)
set.seed(123)

# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_s, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

We now run the K-means algorithm again with two clusters and visualize the clusters in the same variables as before.

```{r}
library(GGally)
# k-means clustering with 2 clusters
km <-kmeans(boston_s, centers = 2)
# plot the boston_s dataset with clusters
pairs(boston_s[6:10],col = km$cluster)
```

Now the clusters seem more meaningful, since those with the highest number of observations are also bigger in area. Some of the black observations, thoough, seem out of place and should be included in the red cluster.